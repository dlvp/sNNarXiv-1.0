{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Imports\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "#from tensorflow.models.rnn.ptb import reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Prepare the data batches </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This whole thing works to learn both abstract and titles. The only difference is that for titles GloVe is trained on an embedding of dimesnion 100, while for abstract the dimension is 512.\n",
    "\n",
    "From : https://github.com/petewarden/tensorflow_makefile/blob/master/tensorflow/models/rnn/ptb/reader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    " \n",
    "import collections\n",
    " \n",
    "def ptb_iterator(raw_data, batch_size, num_steps):\n",
    "\n",
    "  raw_data = np.array(raw_data, dtype=np.int32)\n",
    " \n",
    "  data_len = len(raw_data)\n",
    "  batch_len = data_len // batch_size\n",
    "  data = np.zeros([batch_size, batch_len], dtype=np.int32)\n",
    "  for i in range(batch_size):\n",
    "    data[i] = raw_data[batch_len * i:batch_len * (i + 1)]\n",
    " \n",
    "  epoch_size = (batch_len - 1) // num_steps\n",
    " \n",
    "  if epoch_size == 0:\n",
    "    raise ValueError(\"epoch_size == 0, decrease batch_size or num_steps\")\n",
    " \n",
    "  for i in range(epoch_size):\n",
    "    x = data[:, i*num_steps:(i+1)*num_steps]\n",
    "    y = data[:, i*num_steps+1:(i+1)*num_steps+1]\n",
    "    yield (x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Read the embeddings </h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'vectors.txt' has to be the file containing the right embedding (for abstracts or titles, depending)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "l_key = []\n",
    "l_val=[]\n",
    "with open(\"vectors_abstract.txt\") as f:\n",
    "    for line in f:\n",
    "        line_content = line.split()\n",
    "        key=line_content[0]\n",
    "        val=[float(x) for x in line_content[1:]]\n",
    "        l_key.append(key)\n",
    "        l_val.append(np.asarray(val))\n",
    "dictionary=dict(zip(l_key,l_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'abstract_file' contains the abstract corpus, if trained on title put the title corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('abstract_file', 'rb') as f:\n",
    "    abs_ph_filtered = pickle.load(f)\n",
    "datawords = [item for sublist in abs_ph_filtered for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words_unique = set(datawords)\n",
    "vocab = words_unique\n",
    "vocab_size = len(vocab)\n",
    "idx_to_vocab = dict(enumerate(vocab))\n",
    "vocab_to_idx = dict(zip(idx_to_vocab.values(), idx_to_vocab.keys()))\n",
    "data_temp= [vocab_to_idx[word] for word in datawords]\n",
    "del abs_ph_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_matrix=np.matrix([dictionary[idx_to_vocab[i]] for i in range(vocab_size)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=data_temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Build the TF graph </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Load and process data, utility functions\n",
    "\"\"\"\n",
    "\n",
    "def gen_epochs(n, num_steps, batch_size):\n",
    "    for i in range(n):\n",
    "        yield ptb_iterator(data, batch_size, num_steps)\n",
    "\n",
    "def reset_graph():\n",
    "    if 'sess' in globals() and sess:\n",
    "        sess.close()\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "def train_network(g, num_epochs, num_steps = 200, batch_size = 32, verbose = True, save=False):\n",
    "    tf.set_random_seed(2345)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        training_losses = []\n",
    "        \n",
    "        for idx, epoch in enumerate(gen_epochs(num_epochs, num_steps, batch_size)):\n",
    "            training_loss = 0\n",
    "            steps = 0\n",
    "            training_state = None\n",
    "            for X, Y in epoch:\n",
    "                steps += 1\n",
    "\n",
    "                feed_dict={g['x']: X, g['y']: Y}\n",
    "                if training_state is not None:\n",
    "                    feed_dict[g['init_state']] = training_state\n",
    "                training_loss_, training_state, _ = sess.run([g['total_loss'],\n",
    "                                                      g['final_state'],\n",
    "                                                      g['train_step']],\n",
    "                                                             feed_dict)\n",
    "                training_loss += training_loss_\n",
    "            if (verbose) & (idx % 5==1):\n",
    "                print(\"Average training loss for Epoch\", idx, \":\", training_loss/steps)\n",
    "            training_losses.append(training_loss/steps)\n",
    "\n",
    "            if (idx>1) & (idx % 10==1):\n",
    "                g['saver'].save(sess, save+'_'+str(idx))\n",
    "                \n",
    "        if isinstance(save, str):\n",
    "            g['saver'].save(sess, save)\n",
    "\n",
    "    return training_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding size is 512 for abstracts, probably too big, put 100 for titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_graph(\n",
    "    cell_type = None,\n",
    "    num_weights_for_custom_cell = 5,\n",
    "    state_size = 512, \n",
    "    num_classes = vocab_size,\n",
    "    batch_size = 32,\n",
    "    num_steps = 200,\n",
    "    num_layers = 2,\n",
    "    build_with_dropout=True,\n",
    "    temperature = 1,\n",
    "    learning_rate = 5e-4):\n",
    "\n",
    "    reset_graph()\n",
    "\n",
    "    x = tf.placeholder(tf.int32, [batch_size, num_steps], name='input_placeholder')\n",
    "    y = tf.placeholder(tf.int32, [batch_size, num_steps], name='labels_placeholder')\n",
    "\n",
    "    dropout = tf.constant(0.8)\n",
    "\n",
    "    init_emb = tf.constant(emb_matrix.astype(np.float32))\n",
    "    embeddings = tf.get_variable('embedding_matrix', initializer=init_emb)\n",
    "    rnn_inputs = tf.nn.embedding_lookup(embeddings, x)\n",
    "\n",
    "    if cell_type == 'Custom':\n",
    "        cell = CustomCell(state_size, num_weights_for_custom_cell)\n",
    "    elif cell_type == 'GRU':\n",
    "        cell = tf.nn.rnn_cell.GRUCell(state_size)\n",
    "    elif cell_type == 'LSTM':\n",
    "        cell = tf.nn.rnn_cell.LSTMCell(state_size, state_is_tuple=True)\n",
    "    elif cell_type == 'LN_LSTM':\n",
    "        cell = LayerNormalizedLSTMCell(state_size)\n",
    "    else:\n",
    "        cell = tf.nn.rnn_cell.BasicRNNCell(state_size)\n",
    "\n",
    "    if build_with_dropout:\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, input_keep_prob=dropout)\n",
    "\n",
    "    if cell_type == 'LSTM' or cell_type == 'LN_LSTM':\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers, state_is_tuple=True)\n",
    "    else:\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell([cell] * num_layers)\n",
    "\n",
    "    if build_with_dropout:\n",
    "        cell = tf.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=dropout)\n",
    "\n",
    "    init_state = cell.zero_state(batch_size, tf.float32)\n",
    "    rnn_outputs, final_state = tf.nn.dynamic_rnn(cell, rnn_inputs, initial_state=init_state)\n",
    "\n",
    "    with tf.variable_scope('softmax'):\n",
    "        W = tf.get_variable('W', [state_size, num_classes])\n",
    "        b = tf.get_variable('b', [num_classes], initializer=tf.constant_initializer(0.0))\n",
    "\n",
    "    #reshape rnn_outputs and y\n",
    "    rnn_out_old=rnn_outputs\n",
    "    rnn_outputs = tf.reshape(rnn_outputs, [-1, state_size])\n",
    "    y_reshaped = tf.reshape(y, [-1])\n",
    "\n",
    "    logits = (tf.matmul(rnn_outputs, W) + b)/temperature\n",
    "\n",
    "    predictions = tf.nn.softmax(logits)\n",
    "\n",
    "    softsparse=tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=y_reshaped)\n",
    "    \n",
    "    total_loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits,labels=y_reshaped))\n",
    "    train_step = tf.train.AdamOptimizer(learning_rate).minimize(total_loss)\n",
    "\n",
    "    return dict(\n",
    "        x = x,\n",
    "        y = y,\n",
    "        init_state = init_state,\n",
    "        final_state = final_state,\n",
    "        total_loss = total_loss,\n",
    "        train_step = train_step,\n",
    "        preds = predictions,\n",
    "        saver = tf.train.Saver()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Training </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = build_graph(cell_type='GRU', num_steps=30)\n",
    "t = time.time()\n",
    "losses = train_network(g, 15, num_steps=30, save=\"saves/abstract_epochs\")\n",
    "print(\"It took\", time.time() - t, \"seconds to train for 20 epochs.\")\n",
    "print(\"The average loss on the final epoch was:\", losses[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
